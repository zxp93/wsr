import time
import warnings
from gensim.models import Word2Vec
import pandas as pd
from nltk.corpus import stopwords
import numpy as np
from numpy import float32
import math
from sklearn.ensemble import RandomForestClassifier
import sys
from sklearn.externals import joblib
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer
from sklearn.svm import SVC, LinearSVC
import pickle
from math import *
from sklearn.metrics import classification_report
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import label_binarize
from collections import Counter
from sklearn import random_projection
from sklearn.cluster import KMeans
from tsne import *
from sklearn.model_selection import train_test_split
import metrics
import kmeans_metrics

def cluster_GMM(num_clusters, word_vectors):
    # Initalize a GMM object and use it for clustering.
    clf = GaussianMixture(n_components=num_clusters,
                          covariance_type="tied", init_params='kmeans', max_iter=50)
    # Get cluster assignments.
    clf.fit(word_vectors)
    idx = clf.predict(word_vectors)
    # Get probabilities of cluster assignments.
    idx_proba = clf.predict_proba(word_vectors)
    # Dump cluster assignments and probability of cluster assignments.
    joblib.dump(idx, 'gmm_latestclusmodel_len2alldata.pkl')
    print("Cluster Assignments Saved...")
    joblib.dump(idx_proba, 'gmm_prob_latestclusmodel_len2alldata.pkl')
    print("Probabilities of Cluster Assignments Saved...")
    return (idx, idx_proba)


def read_GMM(idx_name, idx_proba_name):
    # Loads cluster assignments and probability of cluster assignments.
    idx = joblib.load(idx_name)
    idx_proba = joblib.load(idx_proba_name)
    print("Cluster Model Loaded...")
    return (idx, idx_proba)


def get_probability_word_vectors(model, word_centroid_prob_map, num_clusters, word_idf_dict, num_features):
    # This function computes probability word-cluster vectors.
    prob_wordvecs = {}
    for word in word_centroid_prob_map:
        prob_wordvecs[word] = np.zeros(num_clusters * num_features, dtype="float64")
        for index in range(0, num_clusters):
            try:
                prob_wordvecs[word][index * num_features:(index + 1) * num_features] = model[word] * \
                                                                                       word_centroid_prob_map[word][
                                                                                           index] * word_idf_dict[word]
            except:
                continue
    return prob_wordvecs


def read_data():
    f = open('../data/top_10_api.txt', 'r')
    data = f.readlines()
    category = []
    description = []
    for item in data:
        c, d = item.replace('\n', '').split(',')
        category.append(c)
        description.append(d)
    return category, description

def merge_dict(dica, dicb):
    dic = {}
    for key in dica:
        if dicb.get(key):
            dic[key] = dica[key] + dicb[key]
        else:
            dic[key] = dica[key]
    for key in dicb:
        if dica.get(key):
            pass
        else:
            dic[key] = dicb[key]
    return dic


def tag_prob_value():
    category, description = read_data()

    description_list = [d.split(' ') for d in description]
    data = {}
    tag_prob = {}

    all_description_word = []
    for item in description_list:
        all_description_word.extend(item)
    all_word_dict = dict(Counter(all_description_word))
    for item, d in zip(category, description_list):
        c = Counter(d)
        c = dict(c)
        if item not in data.keys():
            data[item] = c
        else:
            data[item] = merge_dict(data[item], c)

    for tag in data.keys():
        if tag not in tag_prob.keys():
            tag_prob[tag] = {}
        for word in all_word_dict.keys():
            try:
                tag_prob[tag][word] = (data[tag][word] + 0.0) / (all_word_dict[word] + 0.0)
            except:
                tag_prob[tag][word] = 0.0

    return tag_prob


def get_tag_word_vectors(model, tag_prob, word_centroid_prob_map, word_idf_dict, num_features):
    # This function computes probability tag-word vectors.
    # num_features is the length of word vector generated by word2vec model
    tag_wordvecs = {}
    length = len(tag_prob.keys())
    for word in word_centroid_prob_map:
        tag_wordvecs[word] = np.zeros(length * num_features, dtype="float32")
        for index, tag in zip(range(0, length), tag_prob.keys()):
            try:
                tag_wordvecs[word][index * num_features:(index + 1) * num_features] = model[word] * tag_prob[tag][
                    word] * word_idf_dict[word]
            except:
                continue
    return tag_wordvecs


def word_idf_value():
    category, description = read_data()

    tfv = TfidfVectorizer(strip_accents='unicode', dtype=np.float32, use_idf=True)
    tfidfmatrix_traindata = tfv.fit_transform(description)
    featurenames = tfv.get_feature_names()
    idf = tfv._tfidf.idf_

    word_idf_dict = {}
    for pair in zip(featurenames, idf):
        word_idf_dict[pair[0]] = pair[1]
    return word_idf_dict


def wsr():
    """
    1.idf value calculating
    2.word vector clustering: aim to get p(c_k|w_i)   k belong to [1,K], K is the number of clusters
    3.document topic-vector formation: obtain dv_i = concat_k{p(c_k|w_i)*wv_i}
    4.tag probability extracting: aim to get p(t_j|w_i)    j belong to [1,J], J is the number of tags
    5.tag vector formation: obtain tv_i = concat_j{p(t_j|w_i)*wv_i}
    6.wsr_i = concat(ov_i,tv_i)
    :return:  wsr vector
    """
    model_name = "../model/word2vec.model"
    num_clusters = 100  # the number of cluster
    num_features = 200  # length of word embedding
    # #######################################################################################################################
    model = Word2Vec.load(model_name)
    word_vectors = model.wv.syn0

    # step 1 >> idf value calculating
    word_idf_dict = word_idf_value()
    # step 2 >> word vector clustering: aim to get p(c_k|w_i)
    idx, idx_proba = cluster_GMM(num_clusters, word_vectors)
    word_centroid_prob_map = dict(zip(model.wv.index2word, idx_proba))
    # step 3 >> document topic-vector formation: obtain dv_i = concat_k{p(c_k|w_i)*wv_i}
    prob_wordvecs = get_probability_word_vectors(model, word_centroid_prob_map, num_clusters, word_idf_dict,
                                                 num_features)
    # step 4 >> tag probability extracting: aim to get p(t_j|w_i)
    tag_prob = tag_prob_value()
    # step 5 >> tag vector formation: obtain tv_i = concat_j{p(t_j|w_i)*wv_i}
    tag_prob_wordvecs = get_tag_word_vectors(model, tag_prob, word_centroid_prob_map, word_idf_dict, num_features)
    # step 6 >> wsr_i = concat(ov_i,tv_i)
    wsr_vector = {}
    for key in prob_wordvecs.keys():
        wsr_vector[key] = np.concatenate((prob_wordvecs[key], tag_prob_wordvecs[key]))
    return wsr_vector


def get_document_vector(wsr_vector, num_feature, num_cluster):
    category, description = read_data()
    length = len(description)
    tag_num = len(set(category))
    doc_vector = [[]] * length
    for index, d in zip(range(length), description):
        wordlist = d.split(' ')
        doc_vector[index] = create_cluster_vector_and_gwbowv(wsr_vector, wordlist, num_feature, num_cluster, tag_num)

    return category, description, np.array(doc_vector)


def filter_data(x_train, x_test):
    global min_no
    global max_no
    percentage = 0.20
    min_no = min_no * 1.0 / len(doc_vector)
    max_no = max_no * 1.0 / len(doc_vector)
    thres = (abs(max_no) + abs(min_no)) / 2
    thres = thres * percentage
    temp = abs(x_train) < thres
    x_train[temp] = 0
    temp = abs(x_test) < thres
    x_test[temp] = 0
    return x_train, x_test


def create_cluster_vector_and_gwbowv(wsr_vector, wordlist, num_features, num_cluster, tag_num, train=True):
    # This function computes SDV feature vectors.
    # dimension == num_features
    bag_of_centroids = np.zeros((num_cluster + tag_num) * num_features, dtype="float32")
    global min_no
    global max_no
    for word in wordlist:
        bag_of_centroids += wsr_vector[word]

    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))
    if (norm != 0):
        bag_of_centroids /= norm

    if train:
        min_no += min(bag_of_centroids)
        max_no += max(bag_of_centroids)

    return bag_of_centroids


if __name__ == '__main__':
    num_clusters = 100  # the number of cluster
    num_features = 200  # length of word embedding
    min_no = 0
    max_no = 0
    start = time.time()
    wsr_vector = wsr()
    category, description, doc_vector = get_document_vector(wsr_vector, num_features, num_clusters)
    print(doc_vector[-1])
    print(max(doc_vector[-1]))
    print(min(doc_vector[-1]))

    mapping = {'Financial': 0.0,
               'Tools': 1.0,
               'Messaging': 2.0,
               'eCommerce': 3.0,
               'Payments': 4.0,
               'Social': 5.0,
               'Enterprise': 6.0,
               'Mapping': 7.0,
               'Science': 8.0,
               'Government': 9.0}

    c = []
    for item in category:
        c.append(mapping[item])
    x_train, x_test, y_train, y_test = train_test_split(doc_vector, c, random_state=2019)

    transformer = random_projection.GaussianRandomProjection(n_components=500, eps=0.0001, random_state=2019)
    x_train = transformer.fit_transform(x_train)
    x_test = transformer.fit_transform(x_test)
    x_train, x_test = filter_data(x_train, x_test)
    print("generating time cost: ", time.time()-start)
    print(x_train.shape)
    print(x_test.shape)

    
    start = time.time()
    param_grid = [
        {'C': np.arange(0.1, 7, 0.2)}]
    # scores = ['accuracy', 'recall_micro', 'f1_micro', 'precision_micro', 'recall_macro', 'f1_macro', 'precision_macro',
    #           'recall_weighted', 'f1_weighted', 'precision_weighted']  # , 'accuracy', 'recall', 'f1']
    scores = ["f1_micro"]
    for score in scores:
        start = time.time()
        print("# Tuning hyper-parameters for", score, "\n")
        clf = GridSearchCV(LinearSVC(C=1, max_iter=1000), param_grid, cv=5, scoring='%s' % score)
        clf.fit(x_train, y_train)
        print("Best parameters set found on development set:\n")
        print(clf.best_params_)
        print("Best value for ", score, ":\n")
        print(clf.best_score_)
        Y_true, Y_pred = y_test, clf.predict(x_test)

        print("Report")
        print(classification_report(Y_true, Y_pred, digits=6))
        print(metrics.purity(Y_true, Y_pred))
        print(metrics.entropy(Y_true, Y_pred))
        print("Accuracy: ", clf.score(x_test, y_test))
        print("Time taken:", time.time() - start, "\n")
    endtime = time.time()
    print("svc time cost: ", time.time()-start)
    print("Total time taken: ", endtime - start, "seconds.")
    print("********************************************************")

    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    from matplotlib.ticker import NullFormatter

    from sklearn import manifold, datasets
    import numpy as np
    import matplotlib
    import matplotlib.pyplot as plt

    matplotlib.use('TkAgg')
    from sklearn.manifold import TSNE



    data = np.concatenate((x_train, x_test), axis=0)
    label = np.concatenate((y_train, y_test), axis=0)
    plot_2D(data, label,'./wsr.png')



    x = np.concatenate((x_train, x_test), axis=0)

    estimator = KMeans(n_clusters=10)
    estimator.fit(x)
    y_p = estimator.labels_
    y_t = np.concatenate((y_train, y_test), axis=0)

    y_t, y_p = kmeans_metrics.kmeans_result_normalize(y_t, y_p)
    print(kmeans_metrics.purity(y_t, y_p))
    print(kmeans_metrics.entropy(y_t, y_p))
    print(classification_report(y_t, y_p))
